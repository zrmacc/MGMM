<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Zachary McCaw" />

<meta name="date" content="2019-01-19" />

<title>Missingness Aware Gaussian Mixture Models</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Missingness Aware Gaussian Mixture Models</h1>
<h4 class="author"><em>Zachary McCaw</em></h4>
<h4 class="date"><em>2019-01-19</em></h4>



<div id="contents" class="section level1">
<h1>Contents</h1>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#data-generation">Data Generation</a></li>
<li><a href="#parameter-estimation">Parameter Estimation</a></li>
</ul>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<div id="overview" class="section level2">
<h2>Overview</h2>
<p>This package implements clustering of multivariate normal random vectors with missing elements. Clustering is achieved by fitting a Gaussian Mixture Model (GMM). The parameters are estimated by maximum likelihood, using the Expectation Maximization (EM) algorithm. Our implementation extends existing methods by allowing for missingness in the input data. The EM algorithm addresses missingness of both the cluster assignments and the vector components. The output includes the marginal cluster membership probabilities; the mean and covariance of each cluster; the density of each mixture component evaluated on the observations; the posterior probabilities of cluster membership; maximum <em>a posteriori</em> cluster assignments; and a completed version of the input data, with missing values replaced by their posterior expectations.</p>
</div>
<div id="model" class="section level2">
<h2>Model</h2>
<p>Suppose the data consist of <span class="math inline">\(n\)</span> random vectors <span class="math inline">\(\{y_{i}\}_{i=1}^{n}\)</span> in <span class="math inline">\(\mathbb{R}^{d}\)</span>. Each observation <span class="math inline">\(y_{i}\)</span> arises from one of <span class="math inline">\(k\)</span> distinct clusters. Associate with each observation a <span class="math inline">\(k\times 1\)</span> vector of indicators <span class="math inline">\(z_{i}\)</span>, where <span class="math inline">\(z_{ij} = 1\)</span> if observation <span class="math inline">\(i\)</span> belongs to cluster <span class="math inline">\(j\)</span>. The marginal probability of membership the <span class="math inline">\(j\)</span>th cluster is <span class="math inline">\(\pi_{j} = P[z_{ij}=1]\)</span>. Conditional on membership to the <span class="math inline">\(j\)</span>th cluster, <span class="math inline">\(y_{i}\)</span> follows a multivariate normal distribution, with mean <span class="math inline">\(\mu_{j}\)</span> and covariance <span class="math inline">\(\Sigma_{j}\)</span>. The generative model is:</p>
<p><span class="math display">\[
z_{i} \sim \text{Multinomial}[\pi_{1},\cdots,\pi_{k}]
\]</span></p>
<p><span class="math display">\[
y_{i}\big|(z_{ij}=1) \sim N\big(\mu_{j},\Sigma_{j}\big)
\]</span></p>
<p>The marginal distribution of the observations is a GMM: <span class="math display">\[
f(y_{i}) = \sum_{j=1}^{k}f(y_{i},z_{ij}=1) = \sum_{j=1}^{k}f(y_{i}|z_{ij}=1)P(z_{ij}=1) = \sum_{j=1}^{k}f(y_{j}|\mu_{j},\Sigma_{j})\pi_{j}
\]</span></p>
<p>Each element <span class="math inline">\(y_{ik}\)</span> of <span class="math inline">\(y_{i}\)</span> is potentially missing at random (MAR). Associated with each observation a <span class="math inline">\(d\times 1\)</span> vector of indicators <span class="math inline">\(R_{i}\)</span>, where <span class="math inline">\(R_{ik}=1\)</span> if element <span class="math inline">\(k\)</span> of observation <span class="math inline">\(i\)</span> is observed. Partition each <span class="math inline">\(y_{i}\)</span> into its observed components <span class="math inline">\(s_{i}\)</span> and its unobserved components <span class="math inline">\(t_{i}\)</span>. That is, <span class="math inline">\(y_{ik}\)</span> belongs to <span class="math inline">\(s_{i}\)</span> if <span class="math inline">\(R_{ik}=1\)</span>, and belongs to <span class="math inline">\(t_{i}\)</span> if <span class="math inline">\(R_{ik}=0\)</span>. The missingness is at random if <span class="math inline">\((R_{ik}\perp y_{ik})|s_{i}\)</span> for each <span class="math inline">\(y_{ik}\)</span> in <span class="math inline">\(t_{i}\)</span>.</p>
<p>Maximum likelihood estimates (MLEs) for the parameters of the GMM are obtained using the EM algorithm. During the E-step, both the cluster assignments <span class="math inline">\(z_{ij}\)</span> and the unobserved components <span class="math inline">\(t_{i}\)</span> of <span class="math inline">\(y_{i}\)</span> are treated as missing data. Suppose temporarily that all data were observed for observation <span class="math inline">\(i\)</span>. The contribution of subject <span class="math inline">\(i\)</span> to the <em>complete data</em> log likelihood would be: <span class="math display">\[
\ell_{i} = \sum_{j=1}^{k}z_{ij}\ln\pi_{j}-\frac{1}{2}\sum_{j=1}^{k}z_{ij}\ln\det(\Sigma_{j})-\frac{1}{2}\sum_{j=1}^{k}z_{ij}(y_{i}-\mu_{j})\Sigma_{j}^{-1}(y_{i}-\mu_{j})
\]</span></p>
<p>Since <span class="math inline">\(z_{ij}\)</span> is never observed, and <span class="math inline">\(y_{i}\)</span> is incompletely observed, the complete data log likelihood cannot be evaluated. In the E-step, the EM objective is formed by taking the expectation of the complete data log likelihood, conditional on the observed data and the current parameter state: <span class="math display">\[
q_{i}^{(r)} \equiv E[\ell_{i}|s_{i},\vartheta^{(r)}] = \sum_{j=1}^{k}\gamma_{ij}^{(r)}\ln\pi_{j}-\frac{1}{2}\sum_{j=1}^{k}\gamma_{ij}^{(r)}\ln\det(\Sigma_{j})-\frac{1}{2}\sum_{j=1}^{k}\text{tr}(\Sigma_{j}^{-1}V_{ij}^{(r)})
\]</span></p>
<p>Here <span class="math inline">\(s_{i}\)</span> is the observed data for subject <span class="math inline">\(i\)</span>; <span class="math inline">\(\vartheta^{(r)}\)</span> is the current parameter state; <span class="math inline">\(\gamma_{ij}^{(r)}\)</span> is the <em>responsibility</em>, defined as <span class="math inline">\(E[z_{ij}|s_{i},\vartheta^{(r)}]\)</span>; and <span class="math inline">\(V_{ij}^{(r)}\)</span> is the <em>expected residual outer product</em>, defined as <span class="math inline">\(E[z_{ij}(y_{i}-\mu_{j})(y_{i}-\mu_{j})'|s_{i},\vartheta^{(r)}]\)</span>. In the M-step, updates of the model parameters <span class="math inline">\(\vartheta\)</span> are obtained by maximizing the EM objective.</p>
<p>Once the convergence criterion has been achieved, the responsibility, or posterior probability of cluster membership, is calculated as: <span class="math display">\[
\gamma_{ij} = P[z_{ij}=1|s_{i}] = \frac{f(s_{i}|\ \mu_{j},\Sigma_{j})\pi_{j}}{\sum_{l=1}^{k}f(s_{i}|\ \mu_{l},\Sigma_{l})\pi_{l}}
\]</span></p>
<p>The maximum a posteriori classification for <span class="math inline">\(y_{i}\)</span> is given by: <span class="math display">\[
A_{i} = \arg\max_{j}\ \gamma_{ij}
\]</span></p>
<p>For observation <span class="math inline">\(y_{i}\)</span>, posterior expectation of the missing elements <span class="math inline">\(t_{i}\)</span>, given the observed elements <span class="math inline">\(s_{i}\)</span>, is: <span class="math display">\[
E[t_{i}|s_{i}] = \sum_{j=1}^{k}E[t_{i}|s_{i},z_{ij}=1]\pi_{j}
\]</span></p>
</div>
</div>
<div id="data-generation" class="section level1">
<h1>Data Generation</h1>
<div id="description" class="section level2">
<h2>Description</h2>
<p>The function <code>rGMM</code> simulates observations from a Gaussian Mixture Model. The number of observations is specified by <code>n</code>, and the dimension of each observation by <code>d</code>. The number of clusters is set using <code>k</code>, which defaults to one. The marginal probabilities of cluster membership are provided as a numeric vector <code>pi</code>, which should contain <code>k</code> elements. If <span class="math inline">\(k&gt;0\)</span> but <code>pi</code> is omitted, the clusters are assumed equi-probable. The proportion of elements in the <span class="math inline">\(n \times d\)</span> data matrix that are missing is specified by <code>m</code>, which defaults to zero. Note that when <span class="math inline">\(m&gt;0\)</span> it is possible for all elements of an observation to be missing. The cluster means <code>M</code> are provided either as a numeric prototype vector, or a list of such vectors. If a single prototype is provided, that vector is used as the mean for all clusters. By default, the zero vector is adopted as the prototype. The cluster covariances <code>S</code> are provided as a numeric prototype matrix, or a list of such matrices. If a single prototype is provided, that matrix is used as the covariance for all clusters. By default, the identity matrix is adopted as the prototype.</p>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<div id="single-component-without-missingness" class="section level3">
<h3>Single Component without Missingness</h3>
<p>In this example, <span class="math inline">\(10^{3}\)</span> observations are simulated from a single <code>k=1</code> bivariate normal distribution <code>d=2</code> without missingness. The mean is <span class="math inline">\(\mu=(2,2)\)</span>, and the covariance is an exchangeable correlation structure with off-diagonal <span class="math inline">\(\rho=0.5\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">100</span>);
<span class="co"># Single component without missingness</span>
Sigma =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="dv">1</span>),<span class="dt">nrow=</span><span class="dv">2</span>);
Y =<span class="st"> </span><span class="kw">rGMM</span>(<span class="dt">n=</span><span class="fl">1e3</span>,<span class="dt">d=</span><span class="dv">2</span>,<span class="dt">k=</span><span class="dv">1</span>,<span class="dt">M=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">S=</span>Sigma);</code></pre></div>
</div>
<div id="single-component-with-missingness" class="section level3">
<h3>Single Component with Missingness</h3>
<p>In this example, <span class="math inline">\(10^{3}\)</span> observations are simulated from a single <code>k=1</code> trivariate normal distribution <code>d=3</code> with 20% missingness <code>m=0.2</code>. The mean defaults to the zero vector, and the covariance to the identity matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Single component with missingness</span>
Y =<span class="st"> </span><span class="kw">rGMM</span>(<span class="dt">n=</span><span class="fl">1e3</span>,<span class="dt">d=</span><span class="dv">3</span>,<span class="dt">k=</span><span class="dv">1</span>,<span class="dt">m=</span><span class="fl">0.2</span>);</code></pre></div>
</div>
<div id="two-components-without-missingness" class="section level3">
<h3>Two Components without Missingness</h3>
<p>In this example, <span class="math inline">\(10^{3}\)</span> observations are simulated from a two-component <code>k=2</code> trivariate normal distribution <code>d=3</code> without missingness. The mean vectors are <span class="math inline">\(\mu_{1}=(-2,-2,-2)\)</span> and <span class="math inline">\(\mu_{2}=(2,2,2)\)</span>. The covariance matrices are both exchangeable with off-diagonal <span class="math inline">\(\rho=0.5\)</span>. Since <code>pi</code> is omitted, the cluster are equi-probable, i.e. <span class="math inline">\(\pi_{1}=\pi_{2}=1/2\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Two-component mixture without missingness</span>
M =<span class="st"> </span><span class="kw">list</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>),<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>));
Sigma =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="dv">1</span>),<span class="dt">nrow=</span><span class="dv">3</span>);
Y =<span class="st"> </span><span class="kw">rGMM</span>(<span class="dt">n=</span><span class="fl">1e3</span>,<span class="dt">d=</span><span class="dv">3</span>,<span class="dt">k=</span><span class="dv">2</span>,<span class="dt">M=</span>M,<span class="dt">S=</span>Sigma);</code></pre></div>
</div>
<div id="four-components-with-missingness" class="section level3">
<h3>Four Components with Missingness</h3>
<p>In this example, <span class="math inline">\(10^{3}\)</span> observations are simulated from a four-component <code>k=4</code> bivariate normal distribution <code>d=2</code> with 10% missingness <code>m=0.1</code>. The mean vectors are <span class="math inline">\(\mu_{1}=(-2,-2)\)</span>, <span class="math inline">\(\mu_{2}=(-2,2)\)</span>, <span class="math inline">\(\mu_{3}=(2,-2)\)</span> and <span class="math inline">\(\mu_{4}=(2,2)\)</span>. The covariance matrices are all <span class="math inline">\(0.5*I\)</span>. The cluster proportions are (35%, 15%, 15%, 35%) for <span class="math inline">\((\pi_{1},\pi_{2},\pi_{3},\pi_{4})\)</span>, respectively.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Four-component mixture with missingness</span>
M =<span class="st"> </span><span class="kw">list</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>),<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>),<span class="kw">c</span>(<span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>),<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>));
S =<span class="st"> </span><span class="fl">0.5</span><span class="op">*</span><span class="kw">diag</span>(<span class="dv">2</span>);
Y =<span class="st"> </span><span class="kw">rGMM</span>(<span class="dt">n=</span><span class="fl">1e3</span>,<span class="dt">d=</span><span class="dv">2</span>,<span class="dt">k=</span><span class="dv">4</span>,<span class="dt">pi=</span><span class="kw">c</span>(<span class="fl">0.35</span>,<span class="fl">0.15</span>,<span class="fl">0.15</span>,<span class="fl">0.35</span>),<span class="dt">m=</span><span class="fl">0.1</span>,<span class="dt">M=</span>M,<span class="dt">S=</span>S);</code></pre></div>
</div>
</div>
</div>
<div id="parameter-estimation" class="section level1">
<h1>Parameter Estimation</h1>
<div id="description-1" class="section level2">
<h2>Description</h2>
<p>The function <code>fit.GMM</code> estimates parameters of the GMM. The data are expected as a numeric matrix <code>Y</code>, with observations as rows. The number of mixture components is specified using <code>k</code>, which defaults to one. Initial values for the mean vectors, covariance matrices, and cluster proportions are provided using <code>M0</code>, <code>S0</code>, and <code>pi0</code>, respectively. If the data <code>Y</code> contain complete observations, i.e. observations with no missing elements, <code>fit.GMM</code> will attempt to initialize the model parameters (<span class="math inline">\(\mu,\Sigma,\pi\)</span>) via K-means. However, if the data <code>Y</code> contain no complete observations, then initial values are required for each of <code>M0</code>, <code>S0</code>, and <code>pi0</code>. Supplying initial values may also result in more accurate estimates when there are relatively few complete observations. The initial means <code>M0</code> are provided as a list of vectors, the covariances <code>S0</code> as a list of matrices, and the cluster proportions <code>pi0</code> as a numeric vector. Note that <code>M0</code> and <code>S0</code> are expected as lists even if the model only contains a single component <code>k=1</code>.</p>
<p>The arguments <code>maxit</code>, <code>eps</code>, <code>report</code>, and <code>parallel</code> control the fitting procedure. <code>maxit</code> sets the maximum number of EM iterations to attempt. The default is <span class="math inline">\(10^{2}\)</span>. <code>eps</code> sets the minimum acceptable improvement in the EM objective function. The default is <span class="math inline">\(10^{-6}\)</span>. If <code>report=TRUE</code>, then fitting progress is displayed. For models with missingness and more than one mixture component <code>k&gt;1</code>, setting <code>parallel=TRUE</code> may improve run time. The parallel backend must be registered beforehand.</p>
</div>
<div id="examples-1" class="section level2">
<h2>Examples</h2>
<div id="single-component-without-missingness-1" class="section level3">
<h3>Single Component without Missingness</h3>
<p>In this example, <span class="math inline">\(10^{3}\)</span> observations are simulated with a single bivariate normal distribution without missingness. Since the model contains only a single component, the output is a list containing the estimated mean and covariance. In the case of a single component without missingness, the maximum likelihood estimates are available in closed form.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Single component without missingness</span>
Sigma =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="dv">1</span>),<span class="dt">nrow=</span><span class="dv">2</span>);
Y =<span class="st"> </span><span class="kw">rGMM</span>(<span class="dt">n=</span><span class="fl">1e3</span>,<span class="dt">d=</span><span class="dv">2</span>,<span class="dt">k=</span><span class="dv">1</span>,<span class="dt">M=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">S=</span>Sigma);
M =<span class="st"> </span><span class="kw">fit.GMM</span>(<span class="dt">Y=</span>Y,<span class="dt">k=</span><span class="dv">1</span>);
<span class="kw">cat</span>(<span class="st">&quot;Estimated Mean and Covariance:</span><span class="ch">\n</span><span class="st">&quot;</span>);
<span class="kw">show</span>(M);</code></pre></div>
<pre><code>## Estimated Mean and Covariance:
## $Mean
##       y1       y2 
## 2.000419 1.975837 
## 
## $Covariance
##           y1        y2
## y1 0.9712543 0.4798659
## y2 0.4798659 1.0434352
## 
## $Objective
## [1] -1756.026</code></pre>
</div>
<div id="single-component-with-missingness-1" class="section level3">
<h3>Single Component with Missingness</h3>
<p>In this example, <span class="math inline">\(10^{3}\)</span> observations are simulated from a single trivariate normal distribution with 20% missingness. Since the model contains only a single component, the output is again a list. However, in the case of missingness, the EM algorithm is used for estimation, and a completed version of the input data is returned, with missing values replaced by their posterior expectations.The true mean is the zero vector, and the true covariance is identity. For <code>M1</code> below, the initial mean and covariance are estimated internally using complete observations. For <code>M2</code> below, the mean and covariance are initialized at the truth. The final value of the EM objective is increased by initializing at the truth.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">101</span>);
<span class="co"># Single component with missingness</span>
Y =<span class="st"> </span><span class="kw">rGMM</span>(<span class="dt">n=</span><span class="fl">1e3</span>,<span class="dt">d=</span><span class="dv">3</span>,<span class="dt">k=</span><span class="dv">1</span>,<span class="dt">m=</span><span class="fl">0.2</span>);
<span class="kw">cat</span>(<span class="st">&quot;Initial parameter values set internally:</span><span class="ch">\n</span><span class="st">&quot;</span>);
M1 =<span class="st"> </span><span class="kw">fit.GMM</span>(<span class="dt">Y=</span>Y,<span class="dt">k=</span><span class="dv">1</span>);
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Estimated mean:</span><span class="ch">\n</span><span class="st">&quot;</span>);
<span class="kw">show</span>(M1<span class="op">$</span>Mean);
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Estimated covariance:</span><span class="ch">\n</span><span class="st">&quot;</span>);
<span class="kw">show</span>(M1<span class="op">$</span>Covariance);
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Final objective:</span><span class="ch">\n</span><span class="st">&quot;</span>);
<span class="kw">show</span>(M1<span class="op">$</span>Objective);
<span class="kw">cat</span>(<span class="st">&quot;Initial parameter values set manually:</span><span class="ch">\n</span><span class="st">&quot;</span>);
m0 =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">3</span>);
S0 =<span class="st"> </span><span class="kw">diag</span>(<span class="dv">3</span>);
M2 =<span class="st"> </span><span class="kw">fit.GMM</span>(<span class="dt">Y=</span>Y,<span class="dt">k=</span><span class="dv">1</span>,<span class="dt">M0=</span><span class="kw">list</span>(m0),<span class="dt">S0=</span><span class="kw">list</span>(S0));
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Estimated mean:</span><span class="ch">\n</span><span class="st">&quot;</span>);
<span class="kw">show</span>(M1<span class="op">$</span>Mean);
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Estimated covariance:</span><span class="ch">\n</span><span class="st">&quot;</span>);
<span class="kw">show</span>(M1<span class="op">$</span>Covariance);
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Final objective:</span><span class="ch">\n</span><span class="st">&quot;</span>);
<span class="kw">show</span>(M1<span class="op">$</span>Objective);
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Gain in final objective by initializing parameters at the truth:</span><span class="ch">\n</span><span class="st">&quot;</span>)
M2<span class="op">$</span>Objective<span class="op">-</span>M1<span class="op">$</span>Objective;</code></pre></div>
<pre><code>## Initial parameter values set internally:
## Objective increment:  6.41 
## Objective increment:  0.633 
## Objective increment:  0.0946 
## Objective increment:  0.0168 
## Objective increment:  0.00312 
## Objective increment:  0.000556 
## Objective increment:  7.71e-05 
## 7 update(s) performed before tolerance limit.
## 
## 
## Estimated mean:
##          y1          y2          y3 
##  0.02821992 -0.01186953 -0.02684324 
## 
## Estimated covariance:
##            y1           y2           y3
## y1 1.02062945  0.069358137  0.021234977
## y2 0.06935814  1.074999999 -0.005636683
## y3 0.02123498 -0.005636683  0.980448577
## 
## Final objective:
## [1] -3034.353
## Initial parameter values set manually:
## Objective increment:  3.97 
## Objective increment:  0.185 
## Objective increment:  0.00439 
## 3 update(s) performed before tolerance limit.
## 
## 
## Estimated mean:
##          y1          y2          y3 
##  0.02821992 -0.01186953 -0.02684324 
## 
## Estimated covariance:
##            y1           y2           y3
## y1 1.02062945  0.069358137  0.021234977
## y2 0.06935814  1.074999999 -0.005636683
## y3 0.02123498 -0.005636683  0.980448577
## 
## Final objective:
## [1] -3034.353
## 
## Gain in final objective by initializing parameters at the truth:
## [1] 0.09823569</code></pre>
</div>
<div id="two-components-without-missingness-1" class="section level3">
<h3>Two Components without Missingness</h3>
<p>In this example, <span class="math inline">\(10^{3}\)</span> observations are simulated from a two-component, trivariate normal distribution without missingness. Since the model has multiple components, the output is an object of class <code>mix</code>. The <em>show method</em> displays the estimated cluster proportions and the final objective. The slots of the output contain the following: * <code>@Means</code> and <code>@Covariances</code>: lists of the estimated cluster means and covariances. * <code>@Density</code>: the cluster densities evaluated at the observations. * <code>@Responsibilities</code>: the posterior membership probabilities for each observation. * <code>@Assignments</code>: the maximum a posteriori cluster assignments. * <code>@Completed</code>: a completed version of the input data is returned, with missing values replaced by their posterior expectations</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Two componets without missingness</span>
Means =<span class="st"> </span><span class="kw">list</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>),<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>));
Sigma =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="dv">1</span>),<span class="dt">nrow=</span><span class="dv">3</span>);
Y =<span class="st"> </span><span class="kw">rGMM</span>(<span class="dt">n=</span><span class="fl">1e3</span>,<span class="dt">d=</span><span class="dv">3</span>,<span class="dt">k=</span><span class="dv">2</span>,<span class="dt">M=</span>Means,<span class="dt">S=</span>Sigma);
M =<span class="st"> </span><span class="kw">fit.GMM</span>(<span class="dt">Y=</span>Y,<span class="dt">k=</span><span class="dv">2</span>,<span class="dt">maxit=</span><span class="dv">10</span>,<span class="dt">eps=</span><span class="fl">1e-8</span>);
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>);
<span class="kw">show</span>(M);
<span class="kw">cat</span>(<span class="st">&quot;Cluster means:</span><span class="ch">\n</span><span class="st">&quot;</span>);
M<span class="op">@</span>Means;
<span class="kw">cat</span>(<span class="st">&quot;Cluster covariances:</span><span class="ch">\n</span><span class="st">&quot;</span>);
M<span class="op">@</span>Covariances;
<span class="kw">cat</span>(<span class="st">&quot;Cluster responsibilities:</span><span class="ch">\n</span><span class="st">&quot;</span>);
<span class="kw">head</span>(M<span class="op">@</span>Responsibilities);
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Cluster assignments:</span><span class="ch">\n</span><span class="st">&quot;</span>);
<span class="kw">head</span>(M<span class="op">@</span>Assignments);</code></pre></div>
<pre><code>## Objective increment:  0.456 
## Objective increment:  0.0227 
## Objective increment:  0.00479 
## Objective increment:  0.00161 
## Objective increment:  0.000641 
## Objective increment:  0.000276 
## Objective increment:  0.000124 
## Objective increment:  5.62e-05 
## Objective increment:  2.58e-05 
## Objective increment:  1.18e-05 
## 10 update(s) performed without reaching tolerance limit.
## 
## 
## Normal Mixture Model with 2 Components. 
## Cluster Proportions:
##    k1    k2 
## 0.491 0.509 
## 
## Final Objective:
##       k1 
## -2828.15 
## 
## Cluster means:
## [[1]]
##        y1        y2        y3 
## -1.968374 -1.977109 -2.022991 
## 
## [[2]]
##       y1       y2       y3 
## 2.004663 1.969927 1.976144 
## 
## Cluster covariances:
## [[1]]
##           y1        y2        y3
## y1 0.9882814 0.5352541 0.5061826
## y2 0.5352541 1.0098347 0.5847462
## y3 0.5061826 0.5847462 1.0150855
## 
## [[2]]
##           y1        y2        y3
## y1 0.8619684 0.3846493 0.3556651
## y2 0.3846493 0.9157168 0.4517638
## y3 0.3556651 0.4517638 0.8882217
## 
## Cluster responsibilities:
##             k1           k2
## 1 9.999937e-01 6.322548e-06
## 2 8.252326e-06 9.999917e-01
## 3 8.250334e-01 1.749666e-01
## 4 9.235427e-08 9.999999e-01
## 5 2.004523e-06 9.999980e-01
## 6 9.999979e-01 2.134687e-06
## 
## Cluster assignments:
##   Assignment
## 1          1
## 2          2
## 3          1
## 4          2
## 5          2
## 6          1</code></pre>
</div>
<div id="four-components-with-missingness-1" class="section level3">
<h3>Four Components with Missingness</h3>
<p>In this example, <span class="math inline">\(10^{3}\)</span> observations are simulated from a four-component bivariate normal distribution with 10% missingness. Since the model has multiple components, the output is an object of class <code>mix</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">200</span>);
<span class="co"># Four components with missingness</span>
M =<span class="st"> </span><span class="kw">list</span>(<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>),<span class="kw">c</span>(<span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>),<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>),<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>));
S =<span class="st"> </span><span class="fl">0.5</span><span class="op">*</span><span class="kw">diag</span>(<span class="dv">2</span>);
Y =<span class="st"> </span><span class="kw">rGMM</span>(<span class="dt">n=</span><span class="dv">1000</span>,<span class="dt">d=</span><span class="dv">2</span>,<span class="dt">k=</span><span class="dv">4</span>,<span class="dt">pi=</span><span class="kw">c</span>(<span class="fl">0.35</span>,<span class="fl">0.15</span>,<span class="fl">0.15</span>,<span class="fl">0.35</span>),<span class="dt">m=</span><span class="fl">0.1</span>,<span class="dt">M=</span>M,<span class="dt">S=</span>S);
M =<span class="st"> </span><span class="kw">fit.GMM</span>(<span class="dt">Y=</span>Y,<span class="dt">k=</span><span class="dv">4</span>,<span class="dt">maxit=</span><span class="dv">10</span>,<span class="dt">eps=</span><span class="fl">1e-8</span>);
<span class="kw">show</span>(M);
<span class="kw">cat</span>(<span class="st">&quot;Cluster means:</span><span class="ch">\n</span><span class="st">&quot;</span>);
M<span class="op">@</span>Means;
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Cluster assignments:</span><span class="ch">\n</span><span class="st">&quot;</span>);
<span class="kw">head</span>(M<span class="op">@</span>Assignments);</code></pre></div>
<pre><code>## Objective increment:  6980 
## Objective increment:  168 
## Objective increment:  90.9 
## Objective increment:  57.4 
## Objective increment:  36.8 
## Objective increment:  26.9 
## Objective increment:  20.6 
## Objective increment:  17.1 
## Objective increment:  15.1 
## Objective increment:  14.1 
## 10 update(s) performed without reaching tolerance limit.
## 
## Normal Mixture Model with 4 Components. 
## Cluster Proportions:
##    k1    k2    k3    k4 
## 0.170 0.303 0.171 0.356 
## 
## Final Objective:
##       k1 
## -3115.16 
## 
## Cluster means:
## [[1]]
##         y1         y2 
## -0.8214704  0.7002373 
## 
## [[2]]
##       y1       y2 
## 2.013224 2.026105 
## 
## [[3]]
##         y1         y2 
##  0.4390973 -0.5140020 
## 
## [[4]]
##        y1        y2 
## -2.063338 -2.027735 
## 
## 
## Cluster assignments:
##   Assignment
## 1          2
## 2          3
## 3          4
## 4          4
## 5          2
## 6          2</code></pre>
</div>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
